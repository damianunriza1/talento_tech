{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP0Ik1xmUdVL21Dcvqeef89"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hcITB2QIjsmH","executionInfo":{"status":"ok","timestamp":1724405959993,"user_tz":300,"elapsed":228,"user":{"displayName":"J. Andres UV","userId":"12207782529429598816"}},"outputId":"d9b4b722-a0fa-4ac9-973d-90d591946700"},"outputs":[{"output_type":"stream","name":"stdout","text":["La acción seleccionada es: derecha\n"]}],"source":["import random # Import the random module\n","\n","class AgenteRL:\n","  def __init__(self, acciones):\n","    self.acciones = acciones\n","\n","\n","  def seleccionar_accion(self, estado):\n","    return random.choice(self.acciones)\n","\n","# Uso del agente RL\n","acciones_posibles = [\"arriba\", \"abajo\", \"izquierda\", \"derecha\"]\n","agente = AgenteRL(acciones_posibles)\n","estado_actual = [0, 0]\n","accion_seleccionada = agente.seleccionar_accion(estado_actual)\n","print(f\"La acción seleccionada es: {accion_seleccionada}\")\n","\n"]},{"cell_type":"code","source":["class EntornoRL:\n","  def __init__(self, estados):\n","    self.estados = estados\n","\n","  def tomar_accion(self, estado, accion): # This method requires two arguments: 'estado' and 'accion'\n","    nuevo_estado = random.choice(self.estados)\n","    recompensa = random.uniform(-10, 10)\n","    return nuevo_estado, recompensa\n","\n","# Uso entorno RL\n","estados_posibles = ['A', 'B', 'C', 'D']\n","entorno = EntornoRL(estados_posibles)\n","estado_actual = 'A' # Define an initial state\n","accion = 'izquierda'\n","nuevo_estado, recompensa = entorno.tomar_accion(estado_actual, accion) # Pass both 'estado_actual' and 'accion' to the method\n","print(\"Nuevo estado: \", nuevo_estado)\n","print(\"Recompensa: \", recompensa)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Rc0fHZUmCQ5","executionInfo":{"status":"ok","timestamp":1724405974432,"user_tz":300,"elapsed":225,"user":{"displayName":"J. Andres UV","userId":"12207782529429598816"}},"outputId":"62d2f793-6ac4-4912-8284-312e390a1dc7"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Nuevo estado:  A\n","Recompensa:  0.7642248054939849\n"]}]},{"cell_type":"code","source":["class QLearning:\n","  def __init__(self, estados, acciones, alpha=0.1, gamma=0.9, epsilon=0.1):\n","    self.estados = estados\n","    self.acciones = acciones\n","    self.alpha = alpha\n","    self.gamma = gamma\n","    self.epsilon = epsilon\n","    self.q_table = {}\n","\n","  def actualizar_q_table(self, estado, accion, recompensa, nuevo_estado):\n","    if estado not in self.q_table:\n","      self.q_table[estado] = {a: 0 for a in self.acciones}\n","\n","    if nuevo_estado not in self.q_table:\n","      self.q_table[nuevo_estado] = {a: 0 for a in self.acciones}\n","      self.q_table[nuevo_estado] = {a: 0 for a in self.acciones}\n","\n","    q_actual = self.q_table[estado_actual][accion]\n","    max_q_nuevo = max(self.q_table[nuevo_estado].values())\n","    nuevo_q_valor = q_actual + self.alpha * (recompensa + self.gamma * max_q_nuevo - q_actual)\n","    self.q_table[estado_actual][accion] = nuevo_q_valor\n","\n","# Uso del algoritmo Q-Learning\n","estados = ['A', 'B', 'C', 'D']\n","acciones = ['izquierda', 'derecha']\n","q_learning = QLearning(estados, acciones)\n","\n","# Simulación de una epoca de entrenamiento\n","estado_actual = 'A'\n","accion = 'izquierda'\n","recompensa = 10\n","nuevo_estado = 'B'\n","q_learning.actualizar_q_table(estado_actual, accion, recompensa, nuevo_estado)\n","\n","# Imprimir la tabla Q\n","print(\"Tabla Q:\")\n","print(q_learning.q_table)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VJFzg8ArmzLX","executionInfo":{"status":"ok","timestamp":1724405988823,"user_tz":300,"elapsed":204,"user":{"displayName":"J. Andres UV","userId":"12207782529429598816"}},"outputId":"59770c8f-ddcd-4962-c8af-a6b7ef5c66d4"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Tabla Q:\n","{'A': {'izquierda': 1.0, 'derecha': 0}, 'B': {'izquierda': 0, 'derecha': 0}}\n"]}]},{"cell_type":"code","source":["# Proceso de Decisión de Markov (MDP)\n","import numpy as np\n","import random\n","\n","# Defincion de estados, acciones y recompensas\n","estados = ['A', 'B', 'C']\n","acciones = ['Arriba', 'Abajo']\n","recompensas = np.random.randint(0, 10, size=(len(estados), len(acciones)))\n","\n","# Función de transición aleatoria\n","def transicion_aleatoria():\n","    return np.random.choice(estados)\n","\n","# Generación de datos\n","estado_actual = np.random.choice(estados)\n","accion = np.random.choice(acciones)\n","nuevo_estado = transicion_aleatoria()\n","recompensa = recompensas[estados.index(estado_actual), acciones.index(accion)]\n","\n","# Imprimir resultados\n","print(f\"Estado actual: {estado_actual}\")\n","print(f\"Acción: {accion}\")\n","print(f\"Nuevo estado: {nuevo_estado}\")\n","print(f\"Recompensa: {recompensa}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C84yKoqMm3XU","executionInfo":{"status":"ok","timestamp":1724406012265,"user_tz":300,"elapsed":209,"user":{"displayName":"J. Andres UV","userId":"12207782529429598816"}},"outputId":"beddc250-45fa-4877-c36b-e5a7b28752c1"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Estado actual: B\n","Acción: Abajo\n","Nuevo estado: C\n","Recompensa: 6\n"]}]},{"cell_type":"code","source":["# Proceso de Decisión de Markov (MDP)\n","import numpy as np\n","import random\n","\n","# Defincion de estados, acciones y recompensas\n","estados = ['A', 'B', 'C']\n","acciones = ['Arriba', 'Abajo']\n","recompensas = np.random.randint(0, 10, size=(len(estados), len(acciones)))\n","\n","# Función de transición aleatoria\n","def transicion_aleatoria():\n","    return np.random.choice(estados)\n","\n","# Generación de datos\n","estado_actual = np.random.choice(estados)\n","accion = np.random.choice(acciones)\n","nuevo_estado = transicion_aleatoria()\n","recompensa = recompensas[estados.index(estado_actual), acciones.index(accion)]\n","\n","# Imprimir resultados\n","print(f\"Estado actual: {estado_actual}\")\n","print(f\"Acción: {accion}\")\n","print(f\"Nuevo estado: {nuevo_estado}\")\n","print(f\"Recompensa: {recompensa}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AlqdPbeum6Fz","executionInfo":{"status":"ok","timestamp":1724406044798,"user_tz":300,"elapsed":213,"user":{"displayName":"J. Andres UV","userId":"12207782529429598816"}},"outputId":"96ece8db-2217-46a5-82e1-f491d0b8f3b9"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Estado actual: B\n","Acción: Abajo\n","Nuevo estado: B\n","Recompensa: 6\n"]}]},{"cell_type":"code","source":["# Conceptos de MDP\n","def calcular_valor_estado(mdp, gamma=0.9, theta=0.01):\n","  valores = {estado: 0 for estado in mdp.estados}\n","  while True:\n","    delta = 0\n","    for estado in mdp.estados:\n","      valor_previo = valores[estado]\n","      valores[estado] = sum(mdp.transiciones[estado][accion][nuevo_estado] * (mdp.recompensas[estado][accion][nuevo_estado] + gamma * valores[nuevo_estado]) for accion, in mdp.acciones for nuevo_estado in mdp.estados)\n","      delta = max(delta, abs(valor_previo - valores[estado]))\n","    if delta < theta:\n","      break\n","  return valores\n","\n","  # Ejemplo de uso\n","  valores_estados = calcular_valor_estado(mdp)\n","  print(\"Valores de los estados: \", valores_estados)"],"metadata":{"id":"dwhUmFpfm_C-","executionInfo":{"status":"ok","timestamp":1724406057223,"user_tz":300,"elapsed":232,"user":{"displayName":"J. Andres UV","userId":"12207782529429598816"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Propiedades de Markov\n","class MDP:\n","    def __init__(self, estados, acciones, transiciones, recompensas):\n","        self.estados = estados\n","        self.acciones = acciones\n","        self.transiciones = transiciones\n","        self.recompensas = recompensas\n","\n","\n","def verificar_propiedad_markov(mdp):\n","  for estado in mdp.estados:\n","    for accion in mdp.acciones:\n","      suma_probabilidades = sum(mdp.transiciones[estado][accion].values())\n","      if not np.isclose(suma_probabilidades, 1):\n","        return False\n","  return True\n","\n","  # ejemplo de uso\n","  print (verificar_propiedad_markov(mdp))\n","\n","transiciones = {\n","    'A': {'Arriba': {'A': 0.3, 'B': 0.7}, 'Abajo': {'C': 1.0}},\n","    'B': {'Arriba': {'A': 0.6, 'B': 0.4}, 'Abajo': {'C': 1.0}},\n","    'C': {'Arriba': {'A': 1.0}, 'Abajo': {'C': 1.0}}\n","}\n","recompensas = {\n","    'A': {'Arriba': {'A': 1, 'B': 2}, 'Abajo': {'C': 3}},\n","    'B': {'Arriba': {'A': 4, 'B': 5}, 'Abajo': {'C': 6}},\n","    'C': {'Arriba': {'A': 7}, 'Abajo': {'C': 8}}\n","}\n","\n","# Crea un objeto MDP\n","mdp = MDP(['A', 'B', 'C'], ['Arriba', 'Abajo'], transiciones, recompensas)\n","\n","if verificar_propiedad_markov(mdp):\n","  print(\"El MDP cumple con la propiedad de Markov.\")\n","else:\n","  print(\"El MDP no cumple con la propiedad de Markov.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ViVAKEK5nGxi","executionInfo":{"status":"ok","timestamp":1724406069408,"user_tz":300,"elapsed":242,"user":{"displayName":"J. Andres UV","userId":"12207782529429598816"}},"outputId":"d7b31255-dcc9-4604-ea57-8357460e7778"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["El MDP cumple con la propiedad de Markov.\n"]}]},{"cell_type":"code","source":["# Propiedad de Recompensa\n","def calcular_recompensa_promedio(mdp):\n","  recompensa_total = 0\n","  total_transiciones = 0\n","  for estado in mdp.estados:\n","    for accion in mdp.acciones:\n","      for nuevo_estado in mdp.transiciones[estado][accion]:\n","        recompensa_total += mdp.transiciones[estado][accion][nuevo_estado] * mdp.recompensas[estado][accion][nuevo_estado]\n","        total_transiciones += mdp.transiciones[estado][accion][nuevo_estado]\n","  return recompensa_total / total_transiciones\n","\n","  # ejemplo de uso\n","  print (\"Recompensa promedio por acción: \", calcular_recompensa_promedio(mdp))"],"metadata":{"id":"878KZP-WnJOX","executionInfo":{"status":"ok","timestamp":1724406079164,"user_tz":300,"elapsed":194,"user":{"displayName":"J. Andres UV","userId":"12207782529429598816"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# recompensa promedio por acción\n","def calcular_recompensa_promedio(mdp):\n","  recompensa_total = 0\n","  total_transiciones = 0\n","  for estado in mdp.estados:\n","    for accion in mdp.acciones:\n","      for nuevo_estado in mdp.transiciones[estado][accion]:\n","        recompensa_total += mdp.transiciones[estado][accion][nuevo_estado] * mdp.recompensas[estado][accion][nuevo_estado]\n","        total_transiciones += mdp.transiciones[estado][accion][nuevo_estado]\n","  return recompensa_total / total_transiciones"],"metadata":{"id":"byf155pGnLjI","executionInfo":{"status":"ok","timestamp":1724406088364,"user_tz":300,"elapsed":228,"user":{"displayName":"J. Andres UV","userId":"12207782529429598816"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"8TsvqmnunBzG"}},{"cell_type":"code","source":["# 1. Introducción a los principales algoritmos de RL:\n","# Define el entorno del juego\n","class Environment:\n","  def __init__(self):\n","    self.state_space = [0, 1, 2, 3]\n","    self.action_space = [0, 1]\n","    self.rewards = {0: -1, 1: -1, 2: -1, 3: -10}\n","\n","# Crea una instancia del entorno\n","env = Environment()\n","\n","# Muestra información del entorno\n","print(\"Estados posibles: \", env.state_space)\n","print(\"Acciones posibles: \", env.action_space)\n","print(\"Recompensas: \", env.rewards)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"myh3OltLnRK9","executionInfo":{"status":"ok","timestamp":1724406117035,"user_tz":300,"elapsed":222,"user":{"displayName":"J. Andres UV","userId":"12207782529429598816"}},"outputId":"329f3490-e317-4558-903c-87efa290b834"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Estados posibles:  [0, 1, 2, 3]\n","Acciones posibles:  [0, 1]\n","Recompensas:  {0: -1, 1: -1, 2: -1, 3: -10}\n"]}]},{"cell_type":"code","source":["# 2. Q-Learning:\n","import numpy as np\n","\n","# Inicializa la tabla Q con valores arbitriarios\n","Q = np.zeros((len(env.state_space), len(env.action_space)))\n","\n","# Define los parametros del algoritmo\n","alpha = 0.1\n","gamma = 0.9\n","epsilon = 0.1\n","# Entrena el agente utilizando Q-learning\n","for _ in range(1000):\n","  state = np.random.choice(env.state_space)\n","  while state != 3:\n","    action = np.random.choice(env.action_space)\n","    next_state = state + action\n","    reward = env.rewards[next_state]\n","    Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n","    state = next_state\n","\n","# Muestra la función Q-valor aprendida\n","print(\"Función Q-valor aprendida: \")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MsmiB3iInUk-","executionInfo":{"status":"ok","timestamp":1724406126524,"user_tz":300,"elapsed":207,"user":{"displayName":"J. Andres UV","userId":"12207782529429598816"}},"outputId":"985b24ba-d565-435d-c852-0062f2052301"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Función Q-valor aprendida: \n","[[ -8.87499891  -9.92105503]\n"," [ -9.9361924   -9.99302418]\n"," [ -9.99405148 -10.        ]\n"," [  0.           0.        ]]\n"]}]},{"cell_type":"code","source":["# 3. Sarsa\n","# Reinicia la tabla Q con valores arbitriarios\n","Q = np.zeros((len(env.state_space), len(env.action_space)))\n","\n","# Entrena el agente usando Sarsa\n","for _ in range(1000):\n","  state = np.random.choice(env.state_space)\n","  action = np.random.choice(env.action_space)\n","  while state != 3:\n","    next_state = state + action\n","    next_action = np.random.choice(env.action_space)\n","    reward = env.rewards[next_state]\n","    Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n","    state = next_state\n","    action = next_action\n","\n","# Muestra la función Q-valor aprendida\n","print(\"Función Q-valor aprendida: \")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24e5mAsbnW_j","executionInfo":{"status":"ok","timestamp":1724406136367,"user_tz":300,"elapsed":270,"user":{"displayName":"J. Andres UV","userId":"12207782529429598816"}},"outputId":"9f062b8d-a65d-4756-bb5d-b42c3ee1df83"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Función Q-valor aprendida: \n","[[ -9.99998513 -10.        ]\n"," [-10.         -10.        ]\n"," [-10.         -10.        ]\n"," [  0.           0.        ]]\n"]}]},{"cell_type":"code","source":["# 4. Política de Gradiente de Montecarlo\n","import numpy as np\n","# Crea una instancia del entorno\n","env = Environment()\n","# Inicia la política con probabilidades uniformes\n","policy = np.ones((len(env.state_space), len(env.action_space))) / len(env.action_space)\n","\n","max_steps = 1000  # Define el límite máximo de pasos\n","steps = 0\n","\n","# Define la función de recompensa promedio\n","def average_reward(Q):\n","  return np.mean([Q[state, np.argmax(policy[state])] for state in env.state_space])\n","\n","# Entrena la política utilizando Gradiente de Montecarlo\n","for _ in range(1000):\n","  state = np.random.choice(env.state_space)\n","  while state != 3 and steps < max_steps:\n","    action = np.random.choice(env.action_space, p=policy[state])\n","    next_state = state + action\n","    reward = env.rewards[next_state]\n","    gradient = np.zeros_like(policy[state])\n","    gradient[action] = 1\n","    alpha = 0.01\n","    policy[state] += alpha * gradient * (reward - average_reward(Q))\n","    steps += 1\n","    # Normalize probabilities to sum to 1\n","    policy[state] /= np.sum(policy[state]) # Add this line to normalize probabilities\n","    state = next_state\n","\n","# Muestra la política aprendida\n","print(\"Política aprendida: \")\n","print(policy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WyeyQX54nZyV","executionInfo":{"status":"ok","timestamp":1724406146989,"user_tz":300,"elapsed":229,"user":{"displayName":"J. Andres UV","userId":"12207782529429598816"}},"outputId":"f8e43c34-4601-4b91-8ac8-338be3a1623c"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Política aprendida: \n","[[4.39187548e-01 5.60812452e-01]\n"," [5.46500244e-01 4.53499756e-01]\n"," [1.00000000e+00 1.88329831e-28]\n"," [5.00000000e-01 5.00000000e-01]]\n"]}]},{"cell_type":"code","source":["# 1: Implementación de Q-Learning en un entorno de gridworld simple\n","import numpy as np\n","\n","# Definición del gridworld\n","gridworld = np.array([\n","  [-1, -1, -1, 1],\n","  [-1, -1, -1, -1],\n","  [-1, -1, -1, -1],\n","  [-1, -1, -1, -1]\n","])\n","\n","# Define las acciones posibles: arriba, abajo, izquierda, derecha\n","actions = {\n","  'up': (-1, 0),\n","  'down': (1, 0),\n","  'left': (0, -1),\n","  'right': (0, 1)\n","}\n","\n","# Implementación de Q-Learning\n","Q = np.zeros((gridworld.shape[0], gridworld.shape[1], len(actions))) # Initialize Q-table correctly\n","\n","gamma = 0.8\n","alpha = 0.1\n","num_episodes = 1000\n","\n","for _ in range(num_episodes):\n","  estado = (0, 0)\n","  while estado != (0, 3):\n","    action_key = np.random.choice(list(actions.keys())) # Choose a random action key\n","    action = actions[action_key] # Get the action tuple from the dictionary\n","    nueva_fila = estado[0] + action[0]\n","    nueva_columna = estado[1] + action[1]\n","    if 0 <= nueva_fila < gridworld.shape[0] and 0 <= nueva_columna < gridworld.shape[1]:\n","      recompensa = gridworld[nueva_fila, nueva_columna]\n","      nuevo_valor = recompensa + gamma * np.max(Q[nueva_fila, nueva_columna])\n","      action_index = list(actions.keys()).index(action_key) # Get the index of the action\n","      Q[estado[0], estado[1], action_index] = (1 - alpha) * Q[estado[0], estado[1], action_index] + alpha * nuevo_valor # Update Q-value for the estado-action pair\n","      estado = (nueva_fila, nueva_columna)\n","\n","# Muestra la tabla Q-valor aprendida\n","print(\"Tabla Q-valor aprendida: \")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5_3zsD08ndm_","executionInfo":{"status":"ok","timestamp":1724406170975,"user_tz":300,"elapsed":2903,"user":{"displayName":"J. Andres UV","userId":"12207782529429598816"}},"outputId":"8dd0519e-83b9-42c3-ae2f-c5ffabb63729"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Tabla Q-valor aprendida: \n","[[[ 0.   -1.    0.   -1.  ]\n","  [ 0.   -1.8  -1.   -0.2 ]\n","  [ 0.   -1.16 -1.    1.  ]\n","  [ 0.    0.    0.    0.  ]]\n","\n"," [[-1.   -1.    0.   -1.8 ]\n","  [-1.   -1.8  -1.   -1.16]\n","  [-0.2  -1.8  -1.8  -0.2 ]\n","  [ 1.   -1.   -1.16  0.  ]]\n","\n"," [[-1.   -1.    0.   -1.8 ]\n","  [-1.8  -1.   -1.   -1.8 ]\n","  [-1.16 -1.   -1.8  -1.  ]\n","  [-0.2  -1.   -1.8   0.  ]]\n","\n"," [[-1.    0.    0.   -1.  ]\n","  [-1.8   0.   -1.   -1.  ]\n","  [-1.8   0.   -1.   -1.  ]\n","  [-1.    0.   -1.    0.  ]]]\n"]}]},{"cell_type":"code","source":["# Ejercicio 2: Aplicación del Aprendizaje por Refuerzo en juegos\n","import numpy as np\n","\n","# Definición de las recompensas\n","recompensas = {'ganar': 10, 'perder': -1, 'empatar': 0}\n","\n","# Implementación de Q-Learning para el juego\n","Q = {}\n","\n","def q_learning(estado_actual, accion, nuevo_estado, resultado):\n","  if (estado_actual, accion) not in Q:\n","    Q[(estado_actual, accion)] = np.zeros(len(acciones))\n","    if nuevo_estado not in Q:\n","      Q[nuevo_estado] = np.zeros(len(acciones))\n","      nuevo_valor = recompensas[resultado] + gamma * np.max(Q[nuevo_estado])\n","      Q[estado_actual][accion] = (1 - alpha) * Q[estado_actual][accion] + alpha * nuevo_valor"],"metadata":{"id":"U6pAWWPVniCl","executionInfo":{"status":"ok","timestamp":1724406181829,"user_tz":300,"elapsed":356,"user":{"displayName":"J. Andres UV","userId":"12207782529429598816"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Ejercicio 3: Aplicación del Aprendizaje por Refuerzo en robótica\n","import numpy as np\n","\n","# Definición del entormo de navegación (datos ficticos)\n","entorno = np.array([\n","  [0, 0, 0, 0, 0],\n","  [0, -1, -1, -1, 0],\n","  [0, 0, -1, 0, 0],\n","  [0, -1, -1, -1, 0],\n","  [0, 0, 0, 0, 0]\n","])\n","\n","# Definición de acciones posibles arriba, abajo, izquierda, derecha\n","acciones = {\n","  'up': (0, -1),\n","  'down': (0, 1),\n","  'left': (-1, 0),\n","  'right': (1, 0)\n","}\n","\n","# Implementación de Q-Learning para el entorno de navegación\n","Q = np.zeros((entorno.shape[0], entorno.shape[1], len(acciones)))\n","\n","gamma = 0.9\n","alpha = 0.1\n","num_episodes = 1000\n","\n","for _ in range(num_episodes):\n","  estado = (0, 0)\n","  while True:\n","    # Choose a random action key instead of an index\n","    accion_key = np.random.choice(list(acciones.keys()))\n","    accion = acciones[accion_key] # Get the action tuple using the key\n","    nueva_fila = estado[0] + accion[0]\n","    nueva_columna = estado[1] + accion[1]\n","    if 0 <= nueva_fila < entorno.shape[0] and 0 <= nueva_columna < entorno.shape[1]:\n","      recompensa = entorno[nueva_fila, nueva_columna]\n","      nuevo_valor = recompensa + gamma * np.max(Q[nueva_fila, nueva_columna])\n","      # Get the index of the action to update the corresponding Q-value\n","      accion_index = list(acciones.keys()).index(accion_key)\n","      Q[estado[0], estado[1], accion_index] = (1 - alpha) * Q[estado[0], estado[1], accion_index] + alpha * nuevo_valor\n","      estado = (nueva_fila, nueva_columna)\n","      if recompensa == -1:\n","        break\n","\n","  # Muestra la tabla Q-valor aprendida\n","print(\"Tabla Q-valor aprendida: \")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H6dVrHsYnlQg","executionInfo":{"status":"ok","timestamp":1724406194241,"user_tz":300,"elapsed":556,"user":{"displayName":"J. Andres UV","userId":"12207782529429598816"}},"outputId":"bc2d62bb-a14a-4047-a208-8c3a0693f314"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Tabla Q-valor aprendida: \n","[[[ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.         -1.        ]\n","  [ 0.          0.          0.         -0.99999926]\n","  [ 0.          0.          0.         -0.99753497]\n","  [ 0.          0.          0.          0.        ]]\n","\n"," [[ 0.         -1.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [-0.61257951  0.          0.          0.        ]]\n","\n"," [[ 0.          0.          0.          0.        ]\n","  [ 0.         -0.95760884 -0.96566316 -0.9774716 ]\n","  [ 0.          0.          0.          0.        ]\n","  [-0.271       0.          0.         -0.1       ]\n","  [ 0.          0.          0.          0.        ]]\n","\n"," [[ 0.         -0.99030226  0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [-0.19        0.          0.          0.        ]]\n","\n"," [[ 0.          0.          0.          0.        ]\n","  [ 0.          0.         -0.86491483  0.        ]\n","  [ 0.          0.         -0.19        0.        ]\n","  [ 0.          0.         -0.1         0.        ]\n","  [ 0.          0.          0.          0.        ]]]\n"]}]},{"cell_type":"code","source":["# Ejercicio 4: Aplicación del Aprendizaje por Refuerzo en gestión de recursos\n","\n","import numpy as np\n","\n","# Definición de los estados (niveles de inventario), acciones (ordenes de reabastecimiento) y recompensas (costos, ganancias, etc.)\n","estados = ['bajo', 'medio', 'alto']\n","acciones = ['reabastecer', 'no_reabastecer']\n","recompensas = {\n","  ('bajo', 'reabastecer'): -50,\n","  ('bajo', 'no_reabastecer'): -10,\n","  ('medio', 'reabastecer'): 30,\n","  ('medio', 'no_reabastecer'): 0,\n","  ('alto', 'reabastecer'): 10,\n","  ('alto', 'no_reabastecer'): -20\n","}\n","\n","# Implementación de Q-Learning\n","Q = {}\n","\n","gamma = 0.9\n","alpha = 0.1\n","num_episodes = 1000\n","\n","for _ in range(num_episodes):\n","  estado_actual = np.random.choice(estados)\n","  while True:\n","    accion = np.random.choice(acciones)\n","    recompensa = recompensas[(estado_actual, accion)]\n","    if (estado_actual, accion) not in Q:\n","      Q[estado_actual] = {}\n","    if accion not in Q[estado_actual]:\n","      Q[estado_actual][accion] = 0\n","      nuevo_estado = np.random.choice(estados)\n","      max_nuevo_estado = max(Q[nuevo_estado].values()) if nuevo_estado in Q else 0\n","      Q[estado_actual][accion] += alpha * (recompensa + gamma * max_nuevo_estado - Q[estado_actual][accion])\n","      estado_actual = nuevo_estado\n","      if recompensa == 50 or recompensa == 30 or recompensa ==10:\n","        break\n","\n","# Muestra la tabla Q-valor aprendida\n","print(\"Tabla Q-valor aprendida: \")\n","print(Q)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nqj8ygi4npIM","executionInfo":{"status":"ok","timestamp":1724406210100,"user_tz":300,"elapsed":565,"user":{"displayName":"J. Andres UV","userId":"12207782529429598816"}},"outputId":"70e99218-3712-4fd7-a2c0-1c749b118c18"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Tabla Q-valor aprendida: \n","{'alto': {'no_reabastecer': -1.7300000000000002}, 'medio': {'reabastecer': 3.0}, 'bajo': {'reabastecer': -4.91}}\n"]}]}]}